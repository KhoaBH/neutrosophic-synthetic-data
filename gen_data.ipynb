{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4081d26c-2618-47a1-b3fa-e094eac4870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ ƒêang ƒë·ªçc d·ªØ li·ªáu v·ªõi Spark...\n",
      "‚úÖ ƒê√£ ƒë·ªçc 768 rows, 9 columns\n",
      "\n",
      "\n",
      "============================================================\n",
      "üîÑ B·∫Øt ƒë·∫ßu sinh synthetic size = 768 (COMPATIBLE MODE)\n",
      "üîÑ T√≠nh to√°n th·ªëng k√™ (compatible mode)...\n",
      "   Converting to pandas for exact skew/kurtosis calculation...\n",
      "üîç Ph√¢n t√≠ch t·ª´ng c·ªôt ƒë·ªÉ ch·ªçn ph√¢n ph·ªëi...\n",
      "----------------------------------------------------------------------\n",
      "üìä Pregnancies     ‚Üí gamma    (skew= 0.90, reason: moderate_skew=0.90_cv=0.88)\n",
      "üìä Glucose         ‚Üí gaussian (skew= 0.17, reason: symmetric_skew=0.17)\n",
      "üìä BloodPressure   ‚Üí gaussian (skew=-1.84, reason: negative_skew=-1.84)\n",
      "üìä SkinThickness   ‚Üí gaussian (skew= 0.11, reason: symmetric_skew=0.11)\n",
      "üìä Insulin         ‚Üí erlang   (skew= 2.27, reason: high_skew=2.27_cv=1.44)\n",
      "üìä BMI             ‚Üí gaussian (skew=-0.43, reason: symmetric_skew=-0.43)\n",
      "üìä DiabetesPedigreeFunction ‚Üí erlang   (skew= 1.92, reason: high_skew=1.92_cv=0.70)\n",
      "üìä Age             ‚Üí gamma    (skew= 1.13, reason: moderate_skew=1.13_cv=0.35)\n",
      "üìä Outcome         ‚Üí erlang   (skew= 0.63, reason: moderate_skew=0.63_high_cv=1.37)\n",
      "----------------------------------------------------------------------\n",
      "üìà Ph√¢n ph·ªëi ƒë∆∞·ª£c ch·ªçn:\n",
      "   Gamma     : 2 c·ªôt(s)\n",
      "   Gaussian  : 4 c·ªôt(s)\n",
      "   Erlang    : 3 c·ªôt(s)\n",
      "\n",
      "üîÑ Sinh 768 m·∫´u synthetic...\n",
      "   Pregnancies: gamma\n",
      "   Glucose: gaussian\n",
      "   BloodPressure: gaussian\n",
      "   SkinThickness: gaussian\n",
      "   Insulin: erlang\n",
      "   BMI: gaussian\n",
      "   DiabetesPedigreeFunction: erlang\n",
      "   Age: gamma\n",
      "   Outcome: erlang\n",
      "üîÑ Fitting Gaussian Copula...\n",
      "üîÑ Sampling t·ª´ copula...\n",
      "üîÑ Transform copula samples...\n",
      "\n",
      "üîÑ X·ª≠ l√Ω binary columns...\n",
      "‚úÖ ƒê√£ x·ª≠ l√Ω binary columns: ['Outcome']\n",
      "‚úÖ ƒê√£ l∆∞u synthetic_compatible_spark_768.csv | Th·ªùi gian: 0m 6s\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "üîÑ B·∫Øt ƒë·∫ßu sinh synthetic size = 100000 (COMPATIBLE MODE)\n",
      "üîÑ T√≠nh to√°n th·ªëng k√™ (compatible mode)...\n",
      "   Converting to pandas for exact skew/kurtosis calculation...\n",
      "üîç Ph√¢n t√≠ch t·ª´ng c·ªôt ƒë·ªÉ ch·ªçn ph√¢n ph·ªëi...\n",
      "----------------------------------------------------------------------\n",
      "üìä Pregnancies     ‚Üí gamma    (skew= 0.90, reason: moderate_skew=0.90_cv=0.88)\n",
      "üìä Glucose         ‚Üí gaussian (skew= 0.17, reason: symmetric_skew=0.17)\n",
      "üìä BloodPressure   ‚Üí gaussian (skew=-1.84, reason: negative_skew=-1.84)\n",
      "üìä SkinThickness   ‚Üí gaussian (skew= 0.11, reason: symmetric_skew=0.11)\n",
      "üìä Insulin         ‚Üí erlang   (skew= 2.27, reason: high_skew=2.27_cv=1.44)\n",
      "üìä BMI             ‚Üí gaussian (skew=-0.43, reason: symmetric_skew=-0.43)\n",
      "üìä DiabetesPedigreeFunction ‚Üí erlang   (skew= 1.92, reason: high_skew=1.92_cv=0.70)\n",
      "üìä Age             ‚Üí gamma    (skew= 1.13, reason: moderate_skew=1.13_cv=0.35)\n",
      "üìä Outcome         ‚Üí erlang   (skew= 0.63, reason: moderate_skew=0.63_high_cv=1.37)\n",
      "----------------------------------------------------------------------\n",
      "üìà Ph√¢n ph·ªëi ƒë∆∞·ª£c ch·ªçn:\n",
      "   Gamma     : 2 c·ªôt(s)\n",
      "   Gaussian  : 4 c·ªôt(s)\n",
      "   Erlang    : 3 c·ªôt(s)\n",
      "\n",
      "üîÑ Sinh 100000 m·∫´u synthetic...\n",
      "   Pregnancies: gamma\n",
      "   Glucose: gaussian\n",
      "   BloodPressure: gaussian\n",
      "   SkinThickness: gaussian\n",
      "   Insulin: erlang\n",
      "   BMI: gaussian\n",
      "   DiabetesPedigreeFunction: erlang\n",
      "   Age: gamma\n",
      "   Outcome: erlang\n",
      "üîÑ Fitting Gaussian Copula...\n",
      "üîÑ Sampling t·ª´ copula...\n",
      "üîÑ Transform copula samples...\n",
      "\n",
      "üîÑ X·ª≠ l√Ω binary columns...\n",
      "‚úÖ ƒê√£ x·ª≠ l√Ω binary columns: ['Outcome']\n",
      "‚úÖ ƒê√£ l∆∞u synthetic_compatible_spark_100000.csv | Th·ªùi gian: 5m 48s\n",
      "============================================================\n",
      "\n",
      "\n",
      "üéâ Ho√†n t·∫•t v·ªõi compatible mode!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, math, time\n",
    "from scipy import stats\n",
    "from scipy.stats import percentileofscore, rankdata, skew, kurtosis\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import Spark\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, udf, broadcast, rand, randn\n",
    "from pyspark.sql.types import DoubleType, ArrayType, StructType, StructField\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Kh·ªüi t·∫°o Spark Session v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u nh∆∞ng consistent\n",
    "def init_spark(app_name=\"SyntheticDataGenerator\"):\n",
    "    \"\"\"Kh·ªüi t·∫°o Spark Session v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    return spark\n",
    "\n",
    "# ----------- GI·ªÆ NGUY√äN C√ÅC H√ÄM PH√ÇN PH·ªêI T·ª™ CODE G·ªêC -----------\n",
    "def fuzz_param(param, IN):\n",
    "    \"\"\"Fuzz parameter v·ªõi noise level IN - GI·ªÆ NGUY√äN\"\"\"\n",
    "    return random.uniform(param*(1-IN), param*(1+IN))\n",
    "\n",
    "def generate_fuzzed_erlang(k, expected_value, IN, size):\n",
    "\n",
    "    theta_mean = k / expected_value  # rate parameter = k/mean\n",
    "    results = []\n",
    "    for _ in range(size):\n",
    "        # Sinh k bi·∫øn exponential v·ªõi rate parameter ƒë∆∞·ª£c fuzz\n",
    "        xN = sum(-math.log(1 - random.uniform(0.001, 0.999)) / fuzz_param(theta_mean, IN) \n",
    "                  for _ in range(k))\n",
    "        results.append(xN)\n",
    "    return np.array(results)\n",
    "\n",
    "def generate_fuzzed_gamma(alpha, beta, IN_alpha, k, size):\n",
    "\n",
    "    IN_beta = k * IN_alpha  # Li√™n k·∫øt noise levels\n",
    "    results = []\n",
    "    for _ in range(size):\n",
    "        # Fuzz alpha v√† beta\n",
    "        alpha_fuzz = fuzz_param(alpha, IN_alpha)\n",
    "        beta_fuzz = max(0.01, fuzz_param(beta, IN_beta))  # ƒê·∫£m b·∫£o beta > 0\n",
    "        \n",
    "        # Sinh t·ª´ Gamma distribution\n",
    "        sample = np.random.gamma(shape=alpha_fuzz, scale=1/beta_fuzz)\n",
    "        results.append(float(sample))\n",
    "    return np.array(results)\n",
    "\n",
    "def generate_fuzzed_gaussian(mu, sigma, IN_sigma, k, size):\n",
    "    \"\"\"GI·ªÆ NGUY√äN 100% logic t·ª´ code g·ªëc\"\"\"\n",
    "    IN_mu = k * IN_sigma  # Li√™n k·∫øt noise levels\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(size // 2 + 1):  # +1 ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªß samples\n",
    "        # Fuzz mu v√† sigma\n",
    "        fuzzed_mu = fuzz_param(mu, IN_mu)\n",
    "        fuzzed_sigma = max(1e-6, fuzz_param(sigma, IN_sigma))  # ƒê·∫£m b·∫£o sigma > 0\n",
    "        \n",
    "        # Sinh 2 bi·∫øn chu·∫©n b·∫±ng Box-Muller\n",
    "        u1, u2 = np.random.uniform(0, 1, 2)\n",
    "        z0 = np.sqrt(-2 * np.log(u1)) * np.cos(2 * np.pi * u2)\n",
    "        z1 = np.sqrt(-2 * np.log(u1)) * np.sin(2 * np.pi * u2)\n",
    "        \n",
    "        data.append(fuzzed_mu + fuzzed_sigma * z0)\n",
    "        data.append(fuzzed_mu + fuzzed_sigma * z1)\n",
    "    \n",
    "    return np.array(data[:size])\n",
    "\n",
    "def transform_uniform_to_distribution(uniform_vals, target_samples):\n",
    "    \"\"\"GI·ªÆ NGUY√äN logic transform t·ª´ code g·ªëc\"\"\"\n",
    "    # T·∫°o empirical CDF t·ª´ target samples\n",
    "    sorted_target = np.sort(target_samples)\n",
    "    n = len(sorted_target)\n",
    "    \n",
    "    # Transform uniform values\n",
    "    transformed = []\n",
    "    for u in uniform_vals:\n",
    "        # T√¨m percentile t∆∞∆°ng ·ª©ng trong target distribution\n",
    "        idx = int(u * (n - 1))\n",
    "        if idx >= n - 1:\n",
    "            transformed.append(sorted_target[-1])\n",
    "        else:\n",
    "            # Linear interpolation\n",
    "            alpha = (u * (n - 1)) - idx\n",
    "            val = sorted_target[idx] * (1 - alpha) + sorted_target[idx + 1] * alpha\n",
    "            transformed.append(val)\n",
    "    \n",
    "    return np.array(transformed)\n",
    "\n",
    "# ----------- TH·ªêNG K√ä T∆Ø∆†NG TH√çCH V·ªöI CODE G·ªêC -----------\n",
    "def compute_column_stats_spark_compatible(spark_df):\n",
    "    \"\"\"\n",
    "    T√≠nh th·ªëng k√™ t∆∞∆°ng th√≠ch 100% v·ªõi code g·ªëc\n",
    "    - Kh√¥ng sample ƒë·ªÉ ƒë·∫£m b·∫£o ch√≠nh x√°c tuy·ªát ƒë·ªëi\n",
    "    - C√πng logic t√≠nh to√°n\n",
    "    \"\"\"\n",
    "    stats_dict = {}\n",
    "    \n",
    "    print(\"üîÑ T√≠nh to√°n th·ªëng k√™ (compatible mode)...\")\n",
    "    \n",
    "    # T√≠nh basic stats b·∫±ng Spark (t·ªëi ∆∞u)\n",
    "    stat_exprs = []\n",
    "    for col_name in spark_df.columns:\n",
    "        stat_exprs.extend([\n",
    "            F.mean(col(col_name)).alias(f\"{col_name}_mean\"),\n",
    "            F.stddev(col(col_name)).alias(f\"{col_name}_std\"),\n",
    "            F.min(col(col_name)).alias(f\"{col_name}_min\"),\n",
    "            F.max(col(col_name)).alias(f\"{col_name}_max\"),\n",
    "            F.count(col(col_name)).alias(f\"{col_name}_count\"),\n",
    "            F.countDistinct(col(col_name)).alias(f\"{col_name}_unique\")\n",
    "        ])\n",
    "    \n",
    "    stats_row = spark_df.select(*stat_exprs).collect()[0]\n",
    "    \n",
    "    # ‚ö†Ô∏è QUAN TR·ªåNG: Convert TO√ÄN B·ªò data ƒë·ªÉ t√≠nh skew/kurtosis ch√≠nh x√°c\n",
    "    # ƒê√¢y l√† ƒëi·ªÉm kh√°c bi·ªát ch√≠nh so v·ªõi optimized version\n",
    "    print(\"   Converting to pandas for exact skew/kurtosis calculation...\")\n",
    "    df_pandas = spark_df.toPandas()\n",
    "    \n",
    "    for col_name in spark_df.columns:\n",
    "        # L·∫•y basic stats t·ª´ Spark\n",
    "        mean_val = stats_row[f\"{col_name}_mean\"]\n",
    "        std_val = stats_row[f\"{col_name}_std\"]\n",
    "        count_val = stats_row[f\"{col_name}_count\"]\n",
    "        unique_val = stats_row[f\"{col_name}_unique\"]\n",
    "        min_val = stats_row[f\"{col_name}_min\"]\n",
    "        max_val = stats_row[f\"{col_name}_max\"]\n",
    "        \n",
    "        # T√≠nh skew/kurtosis t·ª´ TO√ÄN B·ªò data (kh√¥ng sample)\n",
    "        col_data = df_pandas[col_name].values\n",
    "        col_data = col_data[np.isfinite(col_data)]  # Remove NaN/inf\n",
    "        \n",
    "        if len(col_data) > 0:\n",
    "            skew_val = skew(col_data) \n",
    "            kurt_val = kurtosis(col_data)\n",
    "            unique_ratio = unique_val / count_val\n",
    "            cv = std_val / (abs(mean_val) + 1e-8)\n",
    "        else:\n",
    "            skew_val = 0\n",
    "            kurt_val = 0\n",
    "            unique_ratio = 0\n",
    "            cv = 0\n",
    "        \n",
    "        stats_dict[col_name] = {\n",
    "            \"mean\": mean_val,\n",
    "            \"std\": std_val,\n",
    "            \"skewness\": skew_val,\n",
    "            \"kurtosis\": kurt_val,\n",
    "            \"min\": min_val,\n",
    "            \"max\": max_val,\n",
    "            \"unique_ratio\": unique_ratio,\n",
    "            \"cv\": cv\n",
    "        }\n",
    "    \n",
    "    return stats_dict\n",
    "\n",
    "def select_best_distribution(stats_dict, data=None):\n",
    "    \"\"\"GI·ªÆ NGUY√äN 100% logic selection t·ª´ code g·ªëc\"\"\"\n",
    "    skew_val = stats_dict[\"skewness\"]\n",
    "    kurt_val = stats_dict[\"kurtosis\"]\n",
    "    min_val = stats_dict[\"min\"]\n",
    "    cv = stats_dict[\"cv\"]\n",
    "    unique_ratio = stats_dict[\"unique_ratio\"]\n",
    "    std = stats_dict[\"std\"]\n",
    "\n",
    "    # Hard constraints (ch·ªâ gi·ªØ c√°i th·∫≠t s·ª± b·∫Øt bu·ªôc)\n",
    "    if std <= 0:\n",
    "        return \"gaussian\", \"zero_variance\"\n",
    "    if min_val < 0:\n",
    "        return \"gaussian\", \"negative_values\"\n",
    "\n",
    "    # Heuristic rules\n",
    "    if skew_val > 1.5:\n",
    "        if cv > 0.7:\n",
    "            return \"erlang\", f\"high_skew={skew_val:.2f}_cv={cv:.2f}\"\n",
    "        else:\n",
    "            return \"gamma\", f\"high_skew={skew_val:.2f}_low_cv={cv:.2f}\"\n",
    "    elif 0.5 <= skew_val <= 1.5:\n",
    "        if cv > 1:\n",
    "            return \"erlang\", f\"moderate_skew={skew_val:.2f}_high_cv={cv:.2f}\"\n",
    "        else:\n",
    "            return \"gamma\", f\"moderate_skew={skew_val:.2f}_cv={cv:.2f}\"\n",
    "    elif abs(skew_val) < 0.5:\n",
    "        return \"gaussian\", f\"symmetric_skew={skew_val:.2f}\"\n",
    "    elif skew_val < -0.5:\n",
    "        return \"gaussian\", f\"negative_skew={skew_val:.2f}\"\n",
    "    else:\n",
    "        return \"gaussian\", f\"fallback_skew={skew_val:.2f}\"\n",
    "\n",
    "    # Secondary check (n·∫øu mu·ªën v·∫´n note unique_ratio nh∆∞ng kh√¥ng override)\n",
    "    if unique_ratio < 0.1:\n",
    "        return \"gaussian\", f\"low_unique_ratio_skew={skew_val:.2f}\"\n",
    "\n",
    "def auto_select_distributions_spark_compatible(spark_df):\n",
    "    \"\"\"T·ª± ƒë·ªông ch·ªçn ph√¢n ph·ªëi t∆∞∆°ng th√≠ch v·ªõi code g·ªëc\"\"\"\n",
    "    # S·ª≠ d·ª•ng stats function t∆∞∆°ng th√≠ch\n",
    "    column_stats = compute_column_stats_spark_compatible(spark_df)\n",
    "    \n",
    "    distribution_map = {}\n",
    "    print(\"üîç Ph√¢n t√≠ch t·ª´ng c·ªôt ƒë·ªÉ ch·ªçn ph√¢n ph·ªëi...\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for col_name, stats in column_stats.items():\n",
    "        chosen_dist, reason = select_best_distribution(stats)\n",
    "        \n",
    "        analysis = stats.copy()\n",
    "        analysis[\"distribution\"] = chosen_dist\n",
    "        analysis[\"reason\"] = reason\n",
    "        distribution_map[col_name] = analysis\n",
    "        \n",
    "        print(f\"üìä {col_name:15} ‚Üí {chosen_dist:8} \"\n",
    "              f\"(skew={stats['skewness']:5.2f}, reason: {reason})\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Th·ªëng k√™ t·ªïng quan\n",
    "    dist_counts = {}\n",
    "    for info in distribution_map.values():\n",
    "        dist = info['distribution']\n",
    "        dist_counts[dist] = dist_counts.get(dist, 0) + 1\n",
    "    \n",
    "    print(\"üìà Ph√¢n ph·ªëi ƒë∆∞·ª£c ch·ªçn:\")\n",
    "    for dist, count in dist_counts.items():\n",
    "        print(f\"   {dist.capitalize():10}: {count} c·ªôt(s)\")\n",
    "    \n",
    "    return distribution_map\n",
    "\n",
    "# ----------- MAIN FUNCTION T∆Ø∆†NG TH√çCH 100% -----------\n",
    "def generate_adaptive_synthetic_with_spark_compatible(spark_df, global_params=None, column_overrides=None, size=None):\n",
    "    \"\"\"\n",
    "    Version t∆∞∆°ng th√≠ch 100% v·ªõi code g·ªëc\n",
    "    - C√πng logic sinh marginal distributions\n",
    "    - C√πng copula fitting approach  \n",
    "    - C√πng transform method\n",
    "    \"\"\"\n",
    "\n",
    "    if size is None:\n",
    "        size = spark_df.count()\n",
    "    \n",
    "    # Default global parameters - GI·ªÆ NGUY√äN\n",
    "    if global_params is None:\n",
    "        global_params = {\n",
    "            \"erlang\": {\"k\": 2, \"IN\": 0.15},\n",
    "            \"gamma\": {\"k_link\": 0.2, \"IN_alpha\": 0.15}, \n",
    "            \"gaussian\": {\"k_link\": 0.2, \"IN_sigma\": 0.10}\n",
    "        }\n",
    "    \n",
    "    # Step 1: Auto-select distributions cho t·ª´ng c·ªôt - T∆Ø∆†NG TH√çCH\n",
    "    distribution_map = auto_select_distributions_spark_compatible(spark_df)\n",
    "    \n",
    "    # Apply column overrides n·∫øu c√≥\n",
    "    if column_overrides:\n",
    "        for col_name, override_dist in column_overrides.items():\n",
    "            if col_name in distribution_map:\n",
    "                old_dist = distribution_map[col_name]['distribution']\n",
    "                distribution_map[col_name]['distribution'] = override_dist\n",
    "                distribution_map[col_name]['reason'] = f\"manual_override_from_{old_dist}\"\n",
    "                print(f\"üîÑ Override {col_name}: {old_dist} ‚Üí {override_dist}\")\n",
    "    \n",
    "    print(f\"\\nüîÑ Sinh {size} m·∫´u synthetic...\")\n",
    "    \n",
    "    # Convert Spark DataFrame to Pandas ƒë·ªÉ l√†m vi·ªác v·ªõi Copula\n",
    "    # GI·ªÆ NGUY√äN approach n√†y\n",
    "    df_pandas = spark_df.toPandas()\n",
    "    \n",
    "    # Step 2: Sinh marginal distributions cho m·ªói c·ªôt - GI·ªÆ NGUY√äN LOGIC\n",
    "    marginal_samples = {}\n",
    "    \n",
    "    for col_name in df_pandas.columns:\n",
    "        col_data = df_pandas[col_name].values\n",
    "        col_mean = np.mean(col_data)\n",
    "        col_std = np.std(col_data, ddof=1)\n",
    "        dist_info = distribution_map[col_name]\n",
    "        dist_type = dist_info['distribution']\n",
    "        \n",
    "        print(f\"   {col_name}: {dist_type}\")\n",
    "        \n",
    "        if dist_type == \"erlang\":\n",
    "            params = global_params[\"erlang\"]\n",
    "            marginal_samples[col_name] = generate_fuzzed_erlang(\n",
    "                k=params[\"k\"], \n",
    "                expected_value=col_mean, \n",
    "                IN=params[\"IN\"], \n",
    "                size=size\n",
    "            )\n",
    "            \n",
    "        elif dist_type == \"gamma\":\n",
    "            params = global_params[\"gamma\"]\n",
    "            # Fit Gamma: alpha = (mean/std)^2, beta = mean/std^2\n",
    "            if col_std > 0:\n",
    "                alpha = (col_mean / col_std) ** 2\n",
    "                beta = col_mean / (col_std ** 2)\n",
    "            else:\n",
    "                alpha, beta = 1.0, 1.0\n",
    "                \n",
    "            marginal_samples[col_name] = generate_fuzzed_gamma(\n",
    "                alpha=alpha,\n",
    "                beta=beta,\n",
    "                IN_alpha=params[\"IN_alpha\"],\n",
    "                k=params[\"k_link\"],\n",
    "                size=size\n",
    "            )\n",
    "            \n",
    "        elif dist_type == \"gaussian\":\n",
    "            params = global_params[\"gaussian\"]\n",
    "            marginal_samples[col_name] = generate_fuzzed_gaussian(\n",
    "                mu=col_mean,\n",
    "                sigma=max(1e-6, col_std),  # Avoid zero std\n",
    "                IN_sigma=params[\"IN_sigma\"],\n",
    "                k=params[\"k_link\"],\n",
    "                size=size\n",
    "            )\n",
    "        else:\n",
    "            # Fallback to Gaussian\n",
    "            params = global_params[\"gaussian\"]\n",
    "            marginal_samples[col_name] = generate_fuzzed_gaussian(\n",
    "                mu=col_mean,\n",
    "                sigma=max(1e-6, col_std),\n",
    "                IN_sigma=params[\"IN_sigma\"],\n",
    "                k=params[\"k_link\"],\n",
    "                size=size\n",
    "            )\n",
    "    \n",
    "    # Step 3: Fit Gaussian Copula tr√™n d·ªØ li·ªáu g·ªëc - GI·ªÆ NGUY√äN\n",
    "    print(\"üîÑ Fitting Gaussian Copula...\")\n",
    "    copula_model = GaussianMultivariate()\n",
    "    copula_model.fit(df_pandas)\n",
    "    \n",
    "    # Step 4: Sample t·ª´ copula ƒë·ªÉ c√≥ dependency structure - GI·ªÆ NGUY√äN\n",
    "    print(\"üîÑ Sampling t·ª´ copula...\")\n",
    "    copula_samples = copula_model.sample(size)\n",
    "    \n",
    "    # Step 5: Transform copula samples th√†nh target distribution - GI·ªÆ NGUY√äN\n",
    "    print(\"üîÑ Transform copula samples...\")\n",
    "    synthetic_df = pd.DataFrame(index=range(size), columns=df_pandas.columns)\n",
    "    \n",
    "    for col_name in df_pandas.columns:\n",
    "        # L·∫•y copula values cho c·ªôt n√†y\n",
    "        copula_vals = copula_samples[col_name].values\n",
    "        \n",
    "        # Transform v·ªÅ uniform [0,1] b·∫±ng empirical CDF c·ªßa d·ªØ li·ªáu g·ªëc - GI·ªÆ NGUY√äN\n",
    "        uniform_vals = []\n",
    "        for val in copula_vals:\n",
    "            # T√≠nh percentile c·ªßa val trong d·ªØ li·ªáu g·ªëc\n",
    "            percentile = percentileofscore(df_pandas[col_name], val, kind='rank') / 100\n",
    "            uniform_vals.append(percentile)\n",
    "        \n",
    "        uniform_vals = np.array(uniform_vals)\n",
    "        uniform_vals = np.clip(uniform_vals, 0.001, 0.999)  # Tr√°nh extremes\n",
    "        \n",
    "        # Transform uniform values th√†nh marginal distribution - GI·ªÆ NGUY√äN\n",
    "        synthetic_col = transform_uniform_to_distribution(\n",
    "            uniform_vals, \n",
    "            marginal_samples[col_name]\n",
    "        )\n",
    "        synthetic_df[col_name] = synthetic_col\n",
    "    \n",
    "    return synthetic_df, distribution_map\n",
    "\n",
    "# ----------- Utility functions - GI·ªÆ NGUY√äN -----------\n",
    "def read_dataset_spark(spark, path):\n",
    "    \"\"\"ƒê·ªçc CSV dataset b·∫±ng Spark v·ªõi coalesce t·ªëi ∆∞u\"\"\"\n",
    "    return spark.read.csv(path, header=True, inferSchema=True) \\\n",
    "                    .coalesce(spark.sparkContext.defaultParallelism)\n",
    "\n",
    "def detect_binary_columns_spark(spark_df, threshold=0.1):\n",
    "    \"\"\"T·ª± ƒë·ªông ph√°t hi·ªán c·ªôt binary b·∫±ng Spark - GI·ªÆ NGUY√äN LOGIC\"\"\"\n",
    "    binary_cols = []\n",
    "    total_rows = spark_df.count()\n",
    "    \n",
    "    for col_name in spark_df.columns:\n",
    "        unique_count = spark_df.select(col_name).distinct().count()\n",
    "        unique_ratio = unique_count / total_rows\n",
    "        \n",
    "        if unique_ratio <= threshold and unique_count <= 10:\n",
    "            binary_cols.append(col_name)\n",
    "    \n",
    "    return binary_cols\n",
    "\n",
    "def handle_binary_columns(df_synthetic, df_original_pandas, binary_cols=None):\n",
    "    \"\"\"X·ª≠ l√Ω c√°c c·ªôt binary v·ªÅ gi√° tr·ªã discrete - GI·ªÆ NGUY√äN\"\"\"\n",
    "    if binary_cols is None:\n",
    "        binary_cols = []\n",
    "        for col in df_original_pandas.columns:\n",
    "            unique_ratio = df_original_pandas[col].nunique() / len(df_original_pandas)\n",
    "            if unique_ratio <= 0.1 and df_original_pandas[col].nunique() <= 10:\n",
    "                binary_cols.append(col)\n",
    "    \n",
    "    for col in binary_cols:\n",
    "        if col in df_synthetic.columns:\n",
    "            original_values = sorted(df_original_pandas[col].unique())\n",
    "            \n",
    "            if len(original_values) == 2:\n",
    "                # Binary column\n",
    "                threshold = np.median(df_synthetic[col])\n",
    "                df_synthetic[col] = np.where(\n",
    "                    df_synthetic[col] >= threshold, \n",
    "                    original_values[1], \n",
    "                    original_values[0]\n",
    "                )\n",
    "            else:\n",
    "                # Multi-class categorical \n",
    "                synthetic_vals = df_synthetic[col].values\n",
    "                quantized = []\n",
    "                for val in synthetic_vals:\n",
    "                    # T√¨m gi√° tr·ªã g·∫ßn nh·∫•t\n",
    "                    closest_val = min(original_values, key=lambda x: abs(x - val))\n",
    "                    quantized.append(closest_val)\n",
    "                df_synthetic[col] = quantized\n",
    "    \n",
    "    return binary_cols\n",
    "\n",
    "# ----------- Main execution - T∆Ø∆†NG TH√çCH 100% -----------\n",
    "def main():\n",
    "    # Set random seed for reproducibility - QUAN TR·ªåNG!\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Kh·ªüi t·∫°o Spark\n",
    "    spark = init_spark(\"CompatibleSyntheticDataGenerator\")\n",
    "    \n",
    "    try:\n",
    "        # ƒê·ªçc d·ªØ li·ªáu b·∫±ng Spark\n",
    "        print(\"üîÑ ƒêang ƒë·ªçc d·ªØ li·ªáu v·ªõi Spark...\")\n",
    "        spark_df = read_dataset_spark(spark, \"diabetes.csv\")\n",
    "        row_count = spark_df.count()\n",
    "        col_count = len(spark_df.columns)\n",
    "        print(f\" ƒê√£ ƒë·ªçc {row_count} rows, {col_count} columns\\n\")\n",
    "        sizes = [None,100000]\n",
    "        \n",
    "        # L·∫∑p qua t·ª´ng size, sinh v√† l∆∞u + ƒëo th·ªùi gian\n",
    "        for sz in sizes:\n",
    "            # Format t√™n & actual_size\n",
    "            if sz is None:\n",
    "                actual_size = row_count\n",
    "                file_tag = f\"{actual_size}\"\n",
    "            else:\n",
    "                actual_size = int(sz)\n",
    "                file_tag = f\"{actual_size}\"\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üîÑ B·∫Øt ƒë·∫ßu sinh synthetic size = {file_tag} (COMPATIBLE MODE)\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # C·∫£nh b√°o khi size l·ªõn\n",
    "            if actual_size >= 500_000:\n",
    "                print(\"‚ö†Ô∏è  L∆∞u √Ω: size l·ªõn (>=500k). ƒê·∫£m b·∫£o m√°y c√≥ ƒë·ªß RAM ƒë·ªÉ toPandas() v√† x·ª≠ l√Ω.\")\n",
    "            \n",
    "            # G·ªçi h√†m sinh - S·ª¨ D·ª§NG COMPATIBLE VERSION\n",
    "            df_synthetic, distribution_info = generate_adaptive_synthetic_with_spark_compatible(\n",
    "                spark_df,\n",
    "                global_params = {\n",
    "                    \"erlang\": {\"k\": 5, \"IN\": 0.35},        \n",
    "                    \"gamma\": {\"k_link\": 0.3, \"IN_alpha\": 0.35},  \n",
    "                    \"gaussian\": {\"k_link\": 0.3, \"IN_sigma\": 0.35}\n",
    "                },\n",
    "                size = (None if sz is None else int(sz))\n",
    "            )\n",
    "            \n",
    "            # X·ª≠ l√Ω binary columns - GI·ªêNG CODE G·ªêC\n",
    "            df_original_pandas = spark_df.toPandas()\n",
    "            print(\"\\nüîÑ X·ª≠ l√Ω binary columns...\")\n",
    "            binary_cols = handle_binary_columns(df_synthetic, df_original_pandas)\n",
    "            print(f\" ƒê√£ x·ª≠ l√Ω binary columns: {binary_cols}\")\n",
    "            \n",
    "            # L∆∞u file k·∫øt qu·∫£\n",
    "            # output_file = f\"synthetic_adaptive_data_spark35.csv\"\n",
    "            output_file = f\"synthetic_compatible_spark_{file_tag}.csv\"\n",
    "\n",
    "            df_synthetic.to_csv(output_file, index=False)\n",
    "            \n",
    "            # L∆∞u th√¥ng tin distribution ƒë∆∞·ª£c ch·ªçn\n",
    "            dist_summary = pd.DataFrame([\n",
    "                {\n",
    "                    \"Column\": col, \n",
    "                    \"Distribution\": info['distribution'],\n",
    "                    \"Reason\": info['reason'],\n",
    "                    \"Skewness\": info.get('skewness', 'N/A'),\n",
    "                    \"Mean\": info.get('mean', 'N/A'),\n",
    "                    \"Std\": info.get('std', 'N/A')\n",
    "                }\n",
    "                for col, info in distribution_info.items()\n",
    "            ])\n",
    "            dist_summary_file = f\"distribution_compatible_spark_{file_tag}.csv\"\n",
    "            dist_summary.to_csv(dist_summary_file, index=False)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            mins = int(elapsed // 60)\n",
    "            secs = int(elapsed % 60)\n",
    "            print(f\" ƒê√£ l∆∞u {output_file} | Th·ªùi gian: {mins}m {secs}s\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        print(\"\\nüéâ Ho√†n t·∫•t v·ªõi compatible mode!\")\n",
    "        \n",
    "    finally:\n",
    "        # ƒê√≥ng Spark session\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "28f80e7c-108d-45d7-a56c-44d16ea70ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê·ªçc dataset...\n",
      " Dataset: 768 rows x 9 columns\n",
      "\n",
      "============================================================\n",
      "üîÑ Spark Generation: size=768, IN=0.1\n",
      "T√≠nh to√°n th·ªëng k√™ ho√†n to√†n tr√™n Spark (kh√¥ng toPandas)...\n",
      "Stats computed (Spark-native) in 1.06s\n",
      "   Pregnancies: gamma (moderate_skew=0.90_cv=0.88)\n",
      "   Glucose: gaussian (symmetric_skew=0.17)\n",
      "   BloodPressure: gaussian (fallback_skew=-1.84)\n",
      "   SkinThickness: gaussian (symmetric_skew=0.11)\n",
      "   Insulin: erlang (high_skew=2.26_cv=1.44)\n",
      "   BMI: gaussian (symmetric_skew=-0.43)\n",
      "   DiabetesPedigreeFunction: erlang (high_skew=1.91_cv=0.70)\n",
      "   Age: gamma (moderate_skew=1.13_cv=0.35)\n",
      "   Outcome: erlang (moderate_skew=0.63_high_cv=1.37)\n",
      "Sinh t·∫•t c·∫£ marginal distributions trong 1 Spark job...\n",
      " All marginals generated (vectorized) in 1.01s\n",
      "Copula fitted (traditional) in 3.29s\n",
      "‚úÖ Copula sampling & transform in 2.77s\n",
      " Completed in 8.17s | Speed: 94 samples/sec\n",
      "\n",
      "============================================================\n",
      "üîÑ Spark Generation: size=768, IN=0.15\n",
      "T√≠nh to√°n th·ªëng k√™ ho√†n to√†n tr√™n Spark (kh√¥ng toPandas)...\n",
      "Stats computed (Spark-native) in 0.80s\n",
      "   Pregnancies: gamma (moderate_skew=0.90_cv=0.88)\n",
      "   Glucose: gaussian (symmetric_skew=0.17)\n",
      "   BloodPressure: gaussian (fallback_skew=-1.84)\n",
      "   SkinThickness: gaussian (symmetric_skew=0.11)\n",
      "   Insulin: erlang (high_skew=2.26_cv=1.44)\n",
      "   BMI: gaussian (symmetric_skew=-0.43)\n",
      "   DiabetesPedigreeFunction: erlang (high_skew=1.91_cv=0.70)\n",
      "   Age: gamma (moderate_skew=1.13_cv=0.35)\n",
      "   Outcome: erlang (moderate_skew=0.63_high_cv=1.37)\n",
      "Sinh t·∫•t c·∫£ marginal distributions trong 1 Spark job...\n",
      " All marginals generated (vectorized) in 0.47s\n",
      "Copula fitted (traditional) in 3.14s\n",
      "‚úÖ Copula sampling & transform in 2.66s\n",
      " Completed in 7.10s | Speed: 108 samples/sec\n",
      "\n",
      "============================================================\n",
      "üîÑ Spark Generation: size=768, IN=0.2\n",
      "T√≠nh to√°n th·ªëng k√™ ho√†n to√†n tr√™n Spark (kh√¥ng toPandas)...\n",
      "Stats computed (Spark-native) in 0.65s\n",
      "   Pregnancies: gamma (moderate_skew=0.90_cv=0.88)\n",
      "   Glucose: gaussian (symmetric_skew=0.17)\n",
      "   BloodPressure: gaussian (fallback_skew=-1.84)\n",
      "   SkinThickness: gaussian (symmetric_skew=0.11)\n",
      "   Insulin: erlang (high_skew=2.26_cv=1.44)\n",
      "   BMI: gaussian (symmetric_skew=-0.43)\n",
      "   DiabetesPedigreeFunction: erlang (high_skew=1.91_cv=0.70)\n",
      "   Age: gamma (moderate_skew=1.13_cv=0.35)\n",
      "   Outcome: erlang (moderate_skew=0.63_high_cv=1.37)\n",
      "Sinh t·∫•t c·∫£ marginal distributions trong 1 Spark job...\n",
      " All marginals generated (vectorized) in 0.54s\n",
      "Copula fitted (traditional) in 3.19s\n",
      "‚úÖ Copula sampling & transform in 2.97s\n",
      " Completed in 7.39s | Speed: 104 samples/sec\n",
      "\n",
      "============================================================\n",
      "üîÑ Spark Generation: size=768, IN=0.25\n",
      "T√≠nh to√°n th·ªëng k√™ ho√†n to√†n tr√™n Spark (kh√¥ng toPandas)...\n",
      "Stats computed (Spark-native) in 0.79s\n",
      "   Pregnancies: gamma (moderate_skew=0.90_cv=0.88)\n",
      "   Glucose: gaussian (symmetric_skew=0.17)\n",
      "   BloodPressure: gaussian (fallback_skew=-1.84)\n",
      "   SkinThickness: gaussian (symmetric_skew=0.11)\n",
      "   Insulin: erlang (high_skew=2.26_cv=1.44)\n",
      "   BMI: gaussian (symmetric_skew=-0.43)\n",
      "   DiabetesPedigreeFunction: erlang (high_skew=1.91_cv=0.70)\n",
      "   Age: gamma (moderate_skew=1.13_cv=0.35)\n",
      "   Outcome: erlang (moderate_skew=0.63_high_cv=1.37)\n",
      "Sinh t·∫•t c·∫£ marginal distributions trong 1 Spark job...\n",
      " All marginals generated (vectorized) in 0.40s\n",
      "Copula fitted (traditional) in 3.33s\n",
      "‚úÖ Copula sampling & transform in 3.01s\n",
      " Completed in 7.57s | Speed: 101 samples/sec\n",
      "\n",
      "============================================================\n",
      "üîÑ Spark Generation: size=768, IN=0.3\n",
      "T√≠nh to√°n th·ªëng k√™ ho√†n to√†n tr√™n Spark (kh√¥ng toPandas)...\n",
      "Stats computed (Spark-native) in 0.80s\n",
      "   Pregnancies: gamma (moderate_skew=0.90_cv=0.88)\n",
      "   Glucose: gaussian (symmetric_skew=0.17)\n",
      "   BloodPressure: gaussian (fallback_skew=-1.84)\n",
      "   SkinThickness: gaussian (symmetric_skew=0.11)\n",
      "   Insulin: erlang (high_skew=2.26_cv=1.44)\n",
      "   BMI: gaussian (symmetric_skew=-0.43)\n",
      "   DiabetesPedigreeFunction: erlang (high_skew=1.91_cv=0.70)\n",
      "   Age: gamma (moderate_skew=1.13_cv=0.35)\n",
      "   Outcome: erlang (moderate_skew=0.63_high_cv=1.37)\n",
      "Sinh t·∫•t c·∫£ marginal distributions trong 1 Spark job...\n",
      " All marginals generated (vectorized) in 0.51s\n",
      "Copula fitted (traditional) in 3.33s\n",
      "‚úÖ Copula sampling & transform in 2.68s\n",
      " Completed in 7.37s | Speed: 104 samples/sec\n",
      "\n",
      "============================================================\n",
      "üîÑ Spark Generation: size=768, IN=0.35\n",
      "T√≠nh to√°n th·ªëng k√™ ho√†n to√†n tr√™n Spark (kh√¥ng toPandas)...\n",
      "Stats computed (Spark-native) in 0.70s\n",
      "   Pregnancies: gamma (moderate_skew=0.90_cv=0.88)\n",
      "   Glucose: gaussian (symmetric_skew=0.17)\n",
      "   BloodPressure: gaussian (fallback_skew=-1.84)\n",
      "   SkinThickness: gaussian (symmetric_skew=0.11)\n",
      "   Insulin: erlang (high_skew=2.26_cv=1.44)\n",
      "   BMI: gaussian (symmetric_skew=-0.43)\n",
      "   DiabetesPedigreeFunction: erlang (high_skew=1.91_cv=0.70)\n",
      "   Age: gamma (moderate_skew=1.13_cv=0.35)\n",
      "   Outcome: erlang (moderate_skew=0.63_high_cv=1.37)\n",
      "Sinh t·∫•t c·∫£ marginal distributions trong 1 Spark job...\n",
      " All marginals generated (vectorized) in 0.48s\n",
      "Copula fitted (traditional) in 3.57s\n",
      "‚úÖ Copula sampling & transform in 4.81s\n",
      " Completed in 9.61s | Speed: 80 samples/sec\n",
      "\n",
      "üéâ All improved Spark generations completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, math, time\n",
    "from scipy import stats\n",
    "from scipy.stats import percentileofscore, rankdata, skew, kurtosis\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import Spark\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, udf, broadcast, rand, randn, lit, when, array, struct\n",
    "from pyspark.sql.types import DoubleType, ArrayType, StructType, StructField, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# ============================================================================\n",
    "# PH·∫¶N 1: SPARK SESSION OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def init_spark_optimized(app_name=\"OptimizedSparkSynthetic\"):\n",
    "    \"\"\"Spark Session ƒë∆∞·ª£c t·ªëi ∆∞u cho synthetic data generation\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.default.parallelism\", \"16\") \\\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "        .config(\"spark.sql.broadcastTimeout\", \"600\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    return spark\n",
    "\n",
    "# ============================================================================\n",
    "# PH·∫¶N 2: DISTRIBUTION FUNCTIONS - OPTIMIZED FOR SPARK\n",
    "# ============================================================================\n",
    "\n",
    "def fuzz_param(param, IN):\n",
    "    \"\"\"GI·ªÆ NGUY√äN - ƒë∆∞·ª£c s·ª≠ d·ª•ng trong UDF\"\"\"\n",
    "    return random.uniform(param*(1-IN), param*(1+IN))\n",
    "\n",
    "# Broadcast global parameters to all executors\n",
    "def create_optimized_distribution_udfs(spark, global_params):\n",
    "    \"\"\"T·∫°o UDF ƒë∆∞·ª£c t·ªëi ∆∞u v·ªõi broadcast variables\"\"\"\n",
    "    \n",
    "    # Broadcast parameters ƒë·ªÉ tr√°nh serialize nhi·ªÅu l·∫ßn\n",
    "    broadcast_params = spark.sparkContext.broadcast(global_params)\n",
    "    \n",
    "    def erlang_udf_factory(k_val, expected_value_val, IN_val):\n",
    "        @udf(returnType=DoubleType())\n",
    "        def erlang_sample(seed_col):\n",
    "            # S·ª≠ d·ª•ng seed t·ª´ row ƒë·ªÉ ƒë·∫£m b·∫£o reproducibility\n",
    "            random.seed(int(seed_col) + 42)\n",
    "            \n",
    "            params = broadcast_params.value\n",
    "            theta_mean = k_val / expected_value_val\n",
    "            \n",
    "            xN = 0.0\n",
    "            for _ in range(int(k_val)):\n",
    "                u = random.uniform(0.001, 0.999)\n",
    "                theta_fuzzed = fuzz_param(theta_mean, IN_val)\n",
    "                xN += -math.log(u) / theta_fuzzed\n",
    "            \n",
    "            return float(xN)\n",
    "        return erlang_sample\n",
    "    \n",
    "    def gamma_udf_factory(alpha_val, beta_val, IN_alpha_val, k_val):\n",
    "        @udf(returnType=DoubleType())\n",
    "        def gamma_sample(seed_col):\n",
    "            random.seed(int(seed_col) + 42)\n",
    "            np.random.seed(int(seed_col) + 42)\n",
    "            \n",
    "            IN_beta = k_val * IN_alpha_val\n",
    "            alpha_fuzz = fuzz_param(alpha_val, IN_alpha_val)\n",
    "            beta_fuzz = max(0.01, fuzz_param(beta_val, IN_beta))\n",
    "            \n",
    "            # S·ª≠ d·ª•ng numpy gamma\n",
    "            sample = np.random.gamma(shape=alpha_fuzz, scale=1/beta_fuzz)\n",
    "            return float(sample)\n",
    "        return gamma_sample\n",
    "    \n",
    "    def gaussian_udf_factory(mu_val, sigma_val, IN_sigma_val, k_val):\n",
    "        @udf(returnType=DoubleType())\n",
    "        def gaussian_sample(seed_col):\n",
    "            random.seed(int(seed_col) + 42)\n",
    "            \n",
    "            IN_mu = k_val * IN_sigma_val\n",
    "            fuzzed_mu = fuzz_param(mu_val, IN_mu)\n",
    "            fuzzed_sigma = max(1e-6, fuzz_param(sigma_val, IN_sigma_val))\n",
    "            \n",
    "            # Box-Muller transform\n",
    "            u1, u2 = random.uniform(0, 1), random.uniform(0, 1)\n",
    "            z0 = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n",
    "            \n",
    "            return float(fuzzed_mu + fuzzed_sigma * z0)\n",
    "        return gaussian_sample\n",
    "    \n",
    "    return {\n",
    "        \"erlang_factory\": erlang_udf_factory,\n",
    "        \"gamma_factory\": gamma_udf_factory,\n",
    "        \"gaussian_factory\": gaussian_udf_factory\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# PH·∫¶N 3: SPARK-NATIVE STATISTICS - MAJOR IMPROVEMENT\n",
    "# ============================================================================\n",
    "\n",
    "def compute_advanced_stats_spark_native(spark_df):\n",
    "    \"\"\"\n",
    "    MAJOR IMPROVEMENT: T√≠nh skewness v√† kurtosis ho√†n to√†n tr√™n Spark\n",
    "    Kh√¥ng c·∫ßn toPandas() -> ti·∫øt ki·ªám memory v√† tƒÉng t·ªëc ƒë√°ng k·ªÉ\n",
    "    \"\"\"\n",
    "    stats_dict = {}\n",
    "    print(\"T√≠nh to√°n th·ªëng k√™ ho√†n to√†n tr√™n Spark (kh√¥ng toPandas)...\")\n",
    "    \n",
    "    # B∆∞·ªõc 1: T√≠nh basic stats\n",
    "    stat_exprs = []\n",
    "    for col_name in spark_df.columns:\n",
    "        stat_exprs.extend([\n",
    "            F.mean(col(col_name)).alias(f\"{col_name}_mean\"),\n",
    "            F.stddev(col(col_name)).alias(f\"{col_name}_std\"),\n",
    "            F.min(col(col_name)).alias(f\"{col_name}_min\"),\n",
    "            F.max(col(col_name)).alias(f\"{col_name}_max\"),\n",
    "            F.count(col(col_name)).alias(f\"{col_name}_count\"),\n",
    "            F.countDistinct(col(col_name)).alias(f\"{col_name}_unique\")\n",
    "        ])\n",
    "    \n",
    "    basic_stats = spark_df.select(*stat_exprs).collect()[0]\n",
    "    \n",
    "    # B∆∞·ªõc 2: T√≠nh skewness v√† kurtosis tr√™n Spark\n",
    "    for col_name in spark_df.columns:\n",
    "        mean_val = basic_stats[f\"{col_name}_mean\"]\n",
    "        std_val = basic_stats[f\"{col_name}_std\"]\n",
    "        \n",
    "        if std_val > 0:\n",
    "            # T√≠nh skewness: E[((X - Œº)/œÉ)¬≥]\n",
    "            skew_expr = F.mean(\n",
    "                F.pow((col(col_name) - lit(mean_val)) / lit(std_val), 3)\n",
    "            ).alias(f\"{col_name}_skew\")\n",
    "            \n",
    "            # T√≠nh kurtosis: E[((X - Œº)/œÉ)‚Å¥] - 3  \n",
    "            kurt_expr = F.mean(\n",
    "                F.pow((col(col_name) - lit(mean_val)) / lit(std_val), 4)\n",
    "            ).alias(f\"{col_name}_kurt\")\n",
    "            \n",
    "            advanced_stats = spark_df.select(skew_expr, kurt_expr).collect()[0]\n",
    "            skew_val = advanced_stats[f\"{col_name}_skew\"]\n",
    "            kurt_val = advanced_stats[f\"{col_name}_kurt\"] - 3  # Fisher's definition\n",
    "        else:\n",
    "            skew_val = 0\n",
    "            kurt_val = 0\n",
    "        \n",
    "        # Compile final stats\n",
    "        count_val = basic_stats[f\"{col_name}_count\"]\n",
    "        unique_val = basic_stats[f\"{col_name}_unique\"]\n",
    "        min_val = basic_stats[f\"{col_name}_min\"]\n",
    "        max_val = basic_stats[f\"{col_name}_max\"]\n",
    "        \n",
    "        stats_dict[col_name] = {\n",
    "            \"mean\": mean_val,\n",
    "            \"std\": std_val,\n",
    "            \"skewness\": skew_val,\n",
    "            \"kurtosis\": kurt_val,\n",
    "            \"min\": min_val,\n",
    "            \"max\": max_val,\n",
    "            \"unique_ratio\": unique_val / count_val,\n",
    "            \"cv\": std_val / (abs(mean_val) + 1e-8)\n",
    "        }\n",
    "    \n",
    "    return stats_dict\n",
    "\n",
    "def select_best_distribution(stats_dict):\n",
    "    \"\"\"GI·ªÆ NGUY√äN logic selection\"\"\"\n",
    "    skew_val = stats_dict[\"skewness\"]\n",
    "    kurt_val = stats_dict[\"kurtosis\"]\n",
    "    min_val = stats_dict[\"min\"]\n",
    "    cv = stats_dict[\"cv\"]\n",
    "    std = stats_dict[\"std\"]\n",
    "\n",
    "    if std <= 0:\n",
    "        return \"gaussian\", \"zero_variance\"\n",
    "    if min_val < 0:\n",
    "        return \"gaussian\", \"negative_values\"\n",
    "\n",
    "    if skew_val > 1.5:\n",
    "        if cv > 0.7:\n",
    "            return \"erlang\", f\"high_skew={skew_val:.2f}_cv={cv:.2f}\"\n",
    "        else:\n",
    "            return \"gamma\", f\"high_skew={skew_val:.2f}_low_cv={cv:.2f}\"\n",
    "    elif 0.5 <= skew_val <= 1.5:\n",
    "        if cv > 1:\n",
    "            return \"erlang\", f\"moderate_skew={skew_val:.2f}_high_cv={cv:.2f}\"\n",
    "        else:\n",
    "            return \"gamma\", f\"moderate_skew={skew_val:.2f}_cv={cv:.2f}\"\n",
    "    elif abs(skew_val) < 0.5:\n",
    "        return \"gaussian\", f\"symmetric_skew={skew_val:.2f}\"\n",
    "    else:\n",
    "        return \"gaussian\", f\"fallback_skew={skew_val:.2f}\"\n",
    "\n",
    "# ============================================================================\n",
    "# PH·∫¶N 4: VECTORIZED MARGINAL GENERATION ON SPARK\n",
    "# ============================================================================\n",
    "\n",
    "def generate_all_marginals_spark_vectorized(spark, distribution_map, stats_dict, global_params, size):\n",
    "    \"\"\"\n",
    "    MAJOR IMPROVEMENT: Sinh t·∫•t c·∫£ marginal distributions trong 1 Spark job\n",
    "    thay v√¨ nhi·ªÅu jobs ri√™ng bi·ªát\n",
    "    \"\"\"\n",
    "    print(f\"Sinh t·∫•t c·∫£ marginal distributions trong 1 Spark job...\")\n",
    "    \n",
    "    # T·∫°o base DataFrame v·ªõi seed column\n",
    "    base_df = spark.range(size).select(\n",
    "        col(\"id\").alias(\"row_id\"),\n",
    "        (F.lit(42) + col(\"id\")).cast(IntegerType()).alias(\"seed\")\n",
    "    )\n",
    "    \n",
    "    # T·∫°o t·∫•t c·∫£ UDF factories\n",
    "    udf_factories = create_optimized_distribution_udfs(spark, global_params)\n",
    "    \n",
    "    # Build expressions cho t·∫•t c·∫£ columns trong 1 l·∫ßn\n",
    "    column_exprs = [col(\"row_id\")]\n",
    "    \n",
    "    for col_name, dist_info in distribution_map.items():\n",
    "        dist_type = dist_info['distribution']\n",
    "        col_stats = stats_dict[col_name]\n",
    "        \n",
    "        if dist_type == \"erlang\":\n",
    "            params = global_params[\"erlang\"]\n",
    "            udf_func = udf_factories[\"erlang_factory\"](\n",
    "                params[\"k\"], \n",
    "                col_stats[\"mean\"], \n",
    "                params[\"IN\"]\n",
    "            )\n",
    "            column_exprs.append(udf_func(col(\"seed\")).alias(f\"marginal_{col_name}\"))\n",
    "            \n",
    "        elif dist_type == \"gamma\":\n",
    "            params = global_params[\"gamma\"]\n",
    "            col_mean = col_stats[\"mean\"]\n",
    "            col_std = col_stats[\"std\"]\n",
    "            \n",
    "            if col_std > 0:\n",
    "                alpha = (col_mean / col_std) ** 2\n",
    "                beta = col_mean / (col_std ** 2)\n",
    "            else:\n",
    "                alpha, beta = 1.0, 1.0\n",
    "            \n",
    "            udf_func = udf_factories[\"gamma_factory\"](\n",
    "                alpha, beta, \n",
    "                params[\"IN_alpha\"], \n",
    "                params[\"k_link\"]\n",
    "            )\n",
    "            column_exprs.append(udf_func(col(\"seed\")).alias(f\"marginal_{col_name}\"))\n",
    "            \n",
    "        elif dist_type == \"gaussian\":\n",
    "            params = global_params[\"gaussian\"]\n",
    "            udf_func = udf_factories[\"gaussian_factory\"](\n",
    "                col_stats[\"mean\"],\n",
    "                max(1e-6, col_stats[\"std\"]),\n",
    "                params[\"IN_sigma\"],\n",
    "                params[\"k_link\"]\n",
    "            )\n",
    "            column_exprs.append(udf_func(col(\"seed\")).alias(f\"marginal_{col_name}\"))\n",
    "        \n",
    "        else:  # Fallback\n",
    "            params = global_params[\"gaussian\"]\n",
    "            udf_func = udf_factories[\"gaussian_factory\"](\n",
    "                col_stats[\"mean\"],\n",
    "                max(1e-6, col_stats[\"std\"]),\n",
    "                params[\"IN_sigma\"], \n",
    "                params[\"k_link\"]\n",
    "            )\n",
    "            column_exprs.append(udf_func(col(\"seed\")).alias(f\"marginal_{col_name}\"))\n",
    "    \n",
    "    # Execute t·∫•t c·∫£ marginals trong 1 Spark action\n",
    "    marginals_spark_df = base_df.select(*column_exprs)\n",
    "    \n",
    "    # Cache ƒë·ªÉ tr√°nh recomputation\n",
    "    marginals_spark_df.cache()\n",
    "    \n",
    "    # Convert to dictionary format (ch·ªâ collect 1 l·∫ßn)\n",
    "    marginal_data = marginals_spark_df.collect()\n",
    "    \n",
    "    marginal_samples = {}\n",
    "    for original_col in distribution_map.keys():\n",
    "        marginal_col = f\"marginal_{original_col}\"\n",
    "        marginal_samples[original_col] = np.array([\n",
    "            row[marginal_col] for row in marginal_data\n",
    "        ])\n",
    "    \n",
    "    return marginal_samples\n",
    "\n",
    "# ============================================================================\n",
    "# PH·∫¶N 5: SPARK-OPTIMIZED COPULA PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def fit_copula_with_spark_correlation(spark_df):\n",
    "    \"\"\"\n",
    "    S·ª≠ d·ª•ng Spark MLlib ƒë·ªÉ t√≠nh correlation matrix\n",
    "    T·ªëi ∆∞u cho large datasets\n",
    "    \"\"\"\n",
    "    print(\"Fitting copula s·ª≠ d·ª•ng Spark MLlib correlation...\")\n",
    "    \n",
    "    # Chuy·ªÉn th√†nh vector format cho MLlib\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=spark_df.columns,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    vector_df = assembler.transform(spark_df).select(\"features\")\n",
    "    \n",
    "    # T√≠nh correlation matrix b·∫±ng Spark MLlib\n",
    "    corr_matrix = Correlation.corr(vector_df, \"features\", \"pearson\").head()[0]\n",
    "    correlation_array = corr_matrix.toArray()\n",
    "    \n",
    "    # T·∫°o copula model v·ªõi correlation matrix n√†y\n",
    "    copula_model = GaussianMultivariate()\n",
    "    copula_model._correlation = correlation_array\n",
    "    copula_model.columns = list(spark_df.columns)\n",
    "    \n",
    "    return copula_model\n",
    "\n",
    "def transform_uniform_to_distribution(uniform_vals, target_samples):\n",
    "    \"\"\"GI·ªÆ NGUY√äN logic transform\"\"\"\n",
    "    sorted_target = np.sort(target_samples)\n",
    "    n = len(sorted_target)\n",
    "    \n",
    "    transformed = []\n",
    "    for u in uniform_vals:\n",
    "        idx = int(u * (n - 1))\n",
    "        if idx >= n - 1:\n",
    "            transformed.append(sorted_target[-1])\n",
    "        else:\n",
    "            alpha = (u * (n - 1)) - idx\n",
    "            val = sorted_target[idx] * (1 - alpha) + sorted_target[idx + 1] * alpha\n",
    "            transformed.append(val)\n",
    "    \n",
    "    return np.array(transformed)\n",
    "\n",
    "# ============================================================================\n",
    "# PH·∫¶N 6: MAIN IMPROVED FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_adaptive_synthetic_spark_improved(spark_df, global_params=None, column_overrides=None, size=None):\n",
    "    \"\"\"\n",
    "    IMPROVED VERSION: T·ªëi ∆∞u c√°c bottlenecks ch√≠nh trong Spark approach\n",
    "    \n",
    "    Key Improvements:\n",
    "    1. Spark-native skewness/kurtosis computation (no toPandas for stats)\n",
    "    2. Vectorized marginal generation (all columns in 1 Spark job)  \n",
    "    3. MLlib correlation matrix for large datasets\n",
    "    4. Broadcast variables for UDF optimization\n",
    "    5. Proper caching and partition management\n",
    "    \"\"\"\n",
    "    \n",
    "    if size is None:\n",
    "        size = spark_df.count()\n",
    "    \n",
    "    if global_params is None:\n",
    "        global_params = {\n",
    "            \"erlang\": {\"k\": 5, \"IN\": 0.35},\n",
    "            \"gamma\": {\"k_link\": 0.3, \"IN_alpha\": 0.35}, \n",
    "            \"gaussian\": {\"k_link\": 0.3, \"IN_sigma\": 0.35}\n",
    "        }\n",
    "    \n",
    "    spark = spark_df.sql_ctx.sparkSession\n",
    "    \n",
    "    \n",
    "    # Step 1: IMPROVED - Spark-native statistics (major speedup)\n",
    "    t1 = time.time()\n",
    "    stats_dict = compute_advanced_stats_spark_native(spark_df)\n",
    "    print(f\"Stats computed (Spark-native) in {time.time() - t1:.2f}s\")\n",
    "    \n",
    "    # Step 2: Distribution selection - GI·ªÆ NGUY√äN\n",
    "    distribution_map = {}\n",
    "    for col_name, stats in stats_dict.items():\n",
    "        chosen_dist, reason = select_best_distribution(stats)\n",
    "        analysis = stats.copy()\n",
    "        analysis[\"distribution\"] = chosen_dist\n",
    "        analysis[\"reason\"] = reason\n",
    "        distribution_map[col_name] = analysis\n",
    "        print(f\"   {col_name}: {chosen_dist} ({reason})\")\n",
    "    \n",
    "    # Apply overrides\n",
    "    if column_overrides:\n",
    "        for col_name, override_dist in column_overrides.items():\n",
    "            if col_name in distribution_map:\n",
    "                distribution_map[col_name]['distribution'] = override_dist\n",
    "    \n",
    "    # Step 3: IMPROVED - Vectorized marginal generation \n",
    "    t2 = time.time()\n",
    "    marginal_samples = generate_all_marginals_spark_vectorized(\n",
    "        spark, distribution_map, stats_dict, global_params, size\n",
    "    )\n",
    "    print(f\" All marginals generated (vectorized) in {time.time() - t2:.2f}s\")\n",
    "    \n",
    "    # Step 4: IMPROVED - Spark MLlib copula fitting cho large dataset\n",
    "    t3 = time.time()\n",
    "    dataset_size = spark_df.count()\n",
    "    \n",
    "    if dataset_size > 100000:  # Large dataset\n",
    "        copula_model = fit_copula_with_spark_correlation(spark_df)\n",
    "        print(f\"Copula fitted (Spark MLlib) in {time.time() - t3:.2f}s\")\n",
    "    else:  # Small dataset - traditional approach\n",
    "        df_pandas = spark_df.toPandas()\n",
    "        copula_model = GaussianMultivariate()\n",
    "        copula_model.fit(df_pandas)\n",
    "        print(f\"Copula fitted (traditional) in {time.time() - t3:.2f}s\")\n",
    "    \n",
    "    # Step 5: Copula sampling and transform - GI·ªÆ NGUY√äN (kh√¥ng th·ªÉ t·ªëi ∆∞u th√™m)\n",
    "    t4 = time.time()\n",
    "    copula_samples = copula_model.sample(size)\n",
    "    \n",
    "    # Ch·ªâ toPandas() 1 l·∫ßn cho vi·ªác t√≠nh percentile\n",
    "    if dataset_size <= 100000:\n",
    "        df_pandas_ref = df_pandas  # ƒê√£ c√≥ t·ª´ b∆∞·ªõc 4\n",
    "    else:\n",
    "        df_pandas_ref = spark_df.toPandas()  # C·∫ßn toPandas() ƒë·ªÉ t√≠nh percentile\n",
    "    \n",
    "    synthetic_df = pd.DataFrame(index=range(size), columns=spark_df.columns)\n",
    "    \n",
    "    for col_name in spark_df.columns:\n",
    "        copula_vals = copula_samples[col_name].values\n",
    "        \n",
    "        # Transform v·ªÅ uniform [0,1]\n",
    "        uniform_vals = []\n",
    "        for val in copula_vals:\n",
    "            percentile = percentileofscore(df_pandas_ref[col_name], val, kind='rank') / 100\n",
    "            uniform_vals.append(percentile)\n",
    "        \n",
    "        uniform_vals = np.array(uniform_vals)\n",
    "        uniform_vals = np.clip(uniform_vals, 0.001, 0.999)\n",
    "        \n",
    "        # Transform uniform th√†nh target distribution\n",
    "        synthetic_col = transform_uniform_to_distribution(\n",
    "            uniform_vals, \n",
    "            marginal_samples[col_name]\n",
    "        )\n",
    "        synthetic_df[col_name] = synthetic_col\n",
    "    \n",
    "    print(f\"‚úÖ Copula sampling & transform in {time.time() - t4:.2f}s\")\n",
    "    \n",
    "    return synthetic_df, distribution_map\n",
    "\n",
    "# ============================================================================\n",
    "# PH·∫¶N 7: UTILITY & MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def handle_binary_columns_spark(df_synthetic, spark_df_original, binary_cols=None):\n",
    "    \"\"\"GI·ªÆ NGUY√äN logic x·ª≠ l√Ω binary columns\"\"\"\n",
    "    if binary_cols is None:\n",
    "        df_pandas = spark_df_original.toPandas()\n",
    "        binary_cols = []\n",
    "        for col in df_pandas.columns:\n",
    "            unique_ratio = df_pandas[col].nunique() / len(df_pandas)\n",
    "            if unique_ratio <= 0.1 and df_pandas[col].nunique() <= 10:\n",
    "                binary_cols.append(col)\n",
    "    \n",
    "    # X·ª≠ l√Ω binary columns - GI·ªÆ NGUY√äN\n",
    "    df_pandas_original = spark_df_original.toPandas()\n",
    "    for col in binary_cols:\n",
    "        if col in df_synthetic.columns:\n",
    "            original_values = sorted(df_pandas_original[col].unique())\n",
    "            \n",
    "            if len(original_values) == 2:\n",
    "                threshold = np.median(df_synthetic[col])\n",
    "                df_synthetic[col] = np.where(\n",
    "                    df_synthetic[col] >= threshold, \n",
    "                    original_values[1], \n",
    "                    original_values[0]\n",
    "                )\n",
    "            else:\n",
    "                synthetic_vals = df_synthetic[col].values\n",
    "                quantized = []\n",
    "                for val in synthetic_vals:\n",
    "                    closest_val = min(original_values, key=lambda x: abs(x - val))\n",
    "                    quantized.append(closest_val)\n",
    "                df_synthetic[col] = quantized\n",
    "    \n",
    "    return binary_cols\n",
    "\n",
    "def main():\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Kh·ªüi t·∫°o Spark v·ªõi config t·ªëi ∆∞u\n",
    "    spark = init_spark_optimized(\"ImprovedSparkSynthetic\")\n",
    "    \n",
    "    try:\n",
    "        print(\"ƒê·ªçc dataset...\")\n",
    "        spark_df = spark.read.csv(\"diabetes.csv\", header=True, inferSchema=True)\n",
    "        row_count = spark_df.count()\n",
    "        col_count = len(spark_df.columns)\n",
    "        print(f\" Dataset: {row_count} rows x {col_count} columns\")\n",
    "        \n",
    "        sizes = [768]\n",
    "        IN_values = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35]\n",
    "        \n",
    "        for size in sizes:\n",
    "            for IN in IN_values:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\" Spark Generation: size={size}, IN={IN}\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                global_params = {\n",
    "                    \"erlang\": {\"k\": 3, \"IN\": IN},        \n",
    "                    \"gamma\": {\"k_link\": 0.5, \"IN_alpha\": IN},  \n",
    "                    \"gaussian\": {\"k_link\": 0.3, \"IN_sigma\": IN}\n",
    "                }\n",
    "                \n",
    "                df_synthetic, distribution_info = generate_adaptive_synthetic_spark_improved(\n",
    "                    spark_df,\n",
    "                    global_params=global_params,\n",
    "                    size=size\n",
    "                )\n",
    "                \n",
    "                # Handle binary columns\n",
    "                binary_cols = handle_binary_columns_spark(df_synthetic, spark_df)\n",
    "                \n",
    "                # Save results\n",
    "                output_file = f\"synthetic_spark_improved_{size}_{int(IN*100)}.csv\"\n",
    "                df_synthetic.to_csv(output_file, index=False)\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\" Completed in {elapsed:.2f}s | Speed: {size/elapsed:.0f} samples/sec\")\n",
    "        \n",
    "        print(\"\\nüéâ All improved Spark generations completed!\")\n",
    "        \n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e127eb8-e135-4bc2-81d2-f6eb718cfc75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
